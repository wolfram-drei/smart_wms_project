import faiss
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from openai import OpenAI
import re

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key="í‚¤ì…ë ¥")

# =========================
# ì„¤ì •
# =========================
EMBEDDING_DIM = 768
TOP_K = 5  # ê²€ìƒ‰í•  ë¬¸ë‹¨ ê°œìˆ˜
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)
model.eval()

# FAISS ì¸ë±ìŠ¤ì™€ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
index = faiss.read_index("document_faiss_index.idx")
document_titles = np.load("document_titles.npy", allow_pickle=True)
paragraph_texts = np.load("paragraph_texts.npy", allow_pickle=True)

# =========================
# ìœ ì € ì§ˆë¬¸ ì„ë² ë”© í•¨ìˆ˜
# =========================
def embed_query(query):
    inputs = tokenizer(query, return_tensors='pt', truncation=True, padding=True, max_length=512)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    query_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    return query_embedding

# =========================
# ë¬¸ë‹¨ ê²€ìƒ‰ í•¨ìˆ˜
# =========================
def search_documents(query):
    query_vec = embed_query(query)
    distances, indices = index.search(query_vec, TOP_K)
    indices = indices.flatten()
    retrieved_paragraphs = [paragraph_texts[i] for i in indices]
    return retrieved_paragraphs

# =========================
# GPT ë‹µë³€ í¬ë§·íŒ… í•¨ìˆ˜
# =========================
def format_pretty_answer(raw_text):
    """
    GPT RAG ë‹µë³€ì„ ë³´ê¸° ì¢‹ê²Œ Markdown ìŠ¤íƒ€ì¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
    """
    # 1. ëª¨ë“  ì¤„ë°”ê¿ˆ í•˜ë‚˜ë¡œ í†µì¼
    raw_text = re.sub(r'\n+', '\n', raw_text.strip())
    # 2. ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤
    lines = raw_text.split("\n")

    formatted_lines = []
    title_pattern = re.compile(r'^(\*\*)?\d+\.\s+.*(\*\*)?$')  # 1. ~ í˜•íƒœ ì°¾ê¸°
    subtitle_pattern = re.compile(r'^\*\*.*\*\*$')  # **ì†Œì œëª©**
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # ëŒ€ì œëª© (1. ~ ì´ê±°ë‚˜ **1. ~** ì¸ ê²½ìš°)
        if title_pattern.match(line):
            clean_title = line.replace("**", "").strip()
            formatted_lines.append("")
            formatted_lines.append(f"### ğŸ“Œ {clean_title}")
        # ì†Œì œëª© (** ~ **) ì´ì§€ë§Œ ìˆ«ìê°€ ì—†ëŠ” ê²½ìš°
        elif subtitle_pattern.match(line):
            clean_subtitle = line.replace("**", "").strip()
            formatted_lines.append("")
            formatted_lines.append(f"**{clean_subtitle}**")
        # ë¦¬ìŠ¤íŠ¸ í•­ëª© (- ì‹œì‘)
        elif line.startswith("- "):
            formatted_lines.append(f"- {line[2:].strip()}")
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¦¬ìŠ¤íŠ¸ ë³´ì •
        elif any(keyword in line for keyword in ["ì†Œí˜•:", "ì¤‘í˜•:", "ëŒ€í˜•:"]):
            formatted_lines.append(f"- {line}")
        # ê·¸ëƒ¥ ë¬¸ì¥
        else:
            formatted_lines.append(line)
    
    # ë§ˆì§€ë§‰ ì¤„ ì •ë¦¬
    formatted_answer = "\n".join(formatted_lines).strip()

    # <br> ì¹˜í™˜í•´ì„œ í”„ë¡ íŠ¸ì— ë°”ë¡œ ë„˜ê¸´ë‹¤
    formatted_answer = formatted_answer.replace("\n", "<br>")

    return formatted_answer

# =========================
# GPTë¡œ ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜
# =========================
def generate_rag_answer(user_query):
    retrieved_paragraphs = search_documents(user_query)
    prompt = f"""
        ë‹¹ì‹ ì€ WMS ì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.
        ì•„ë˜ëŠ” ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë¬¸ë‹¨ì…ë‹ˆë‹¤:
        ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.
        {retrieved_paragraphs}

        ğŸ”¥ ë‹µë³€ ì‘ì„± ì§€ì¹¨:
        - ì¹œì ˆí•˜ê³  ì •ë¦¬ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
        - ì£¼ìš” í•­ëª©ì´ ë°”ë€” ë•Œë§ˆë‹¤ **\\n\\n** (ì¤„ë°”ê¿ˆ ë‘ ë²ˆ)ì„ ì‚½ì…í•˜ì„¸ìš”.
        - ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•œ ê²½ìš° **ë°˜ë“œì‹œ - (í•˜ì´í”ˆ)** ìœ¼ë¡œ ì‹œì‘í•´ì„œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”. (ì˜ˆ: "- ì†Œí˜•: 8ê°œ ì ì¬")
        - ë¦¬ìŠ¤íŠ¸ í•˜ì´í”ˆ(-) ëˆ„ë½ ê¸ˆì§€
        - ë¶ˆí•„ìš”í•œ ë§ì€ ë„£ì§€ ë§ê³  ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ì„¸ìš”.
        (ì¤„ë°”ê¿ˆ ëŠë‚Œë§Œ ë‚´ì§€ ë§ê³ , ì‹¤ì œë¡œ '\\n\\n' ì¤„ë°”ê¿ˆ 2ê°œë¥¼ ì‚½ì…í•˜ì„¸ìš”!)
        
        ìœ„ ë¬¸ë‹¨ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹µë³€ì€ í¬ë§·íŒ…ì„ ê³ ë ¤í•´ì„œ ì¹œì ˆí•˜ê³  ë³´ê¸° ì¢‹ê²Œ ì‘ì„±í•´
        ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}"
        ë‹µë³€:
        """
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” WMS ì‹œìŠ¤í…œ ê´€ë ¨ ì „ë¬¸ ì•ˆë‚´ ì±—ë´‡ì´ì•¼."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0,
            max_tokens=1500
        )
        answer = response.choices[0].message.content.strip()
        pretty_answer = format_pretty_answer(answer)
        return pretty_answer
    except Exception as e:
        print(f"[GPT ì˜¤ë¥˜] {e}")
        return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

